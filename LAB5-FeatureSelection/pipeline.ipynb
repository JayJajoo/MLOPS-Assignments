{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88757393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine_feature_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "class FeatureSelectionPipeline:\n",
    "    def __init__(self, df, target):\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.X = df.drop(columns=[target])\n",
    "        self.y = df[target]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.results = pd.DataFrame()\n",
    "        self.selected_features = {}\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Scale numeric features\n",
    "        self.X_scaled = pd.DataFrame(\n",
    "            self.scaler.fit_transform(self.X), \n",
    "            columns=self.X.columns\n",
    "        )\n",
    "        print(\"Preprocessing done. Features scaled.\")\n",
    "\n",
    "    def filter_methods(self, top_k=8):\n",
    "        # Univariate feature selection\n",
    "        selector = SelectKBest(score_func=f_classif, k=top_k)\n",
    "        selector.fit(self.X_scaled, self.y)\n",
    "        selected = self.X_scaled.columns[selector.get_support()].tolist()\n",
    "        self.selected_features['Filter_SelectKBest'] = selected\n",
    "        print(f\"Filter method selected features: {selected}\")\n",
    "\n",
    "    def wrapper_methods(self):\n",
    "        # Recursive Feature Elimination\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        rfe = RFE(model, n_features_to_select=8)\n",
    "        rfe.fit(self.X_scaled, self.y)\n",
    "        selected = self.X_scaled.columns[rfe.support_].tolist()\n",
    "        self.selected_features['Wrapper_RFE'] = selected\n",
    "        print(f\"Wrapper method selected features: {selected}\")\n",
    "\n",
    "    def embedded_methods(self):\n",
    "        # Random Forest feature importance\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        model.fit(self.X_scaled, self.y)\n",
    "        importances = pd.Series(model.feature_importances_, index=self.X_scaled.columns)\n",
    "        selected = importances.sort_values(ascending=False).head(8).index.tolist()\n",
    "        self.selected_features['Embedded_RandomForest'] = selected\n",
    "        print(f\"Embedded method selected features: {selected}\")\n",
    "\n",
    "        # Optional: XGBoost feature importance\n",
    "        model_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model_xgb.fit(self.X_scaled, self.y)\n",
    "        importances_xgb = pd.Series(model_xgb.feature_importances_, index=self.X_scaled.columns)\n",
    "        selected_xgb = importances_xgb.sort_values(ascending=False).head(8).index.tolist()\n",
    "        self.selected_features['Embedded_XGBoost'] = selected_xgb\n",
    "        print(f\"Embedded XGBoost selected features: {selected_xgb}\")\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        # Split dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X_scaled, self.y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        metrics_list = []\n",
    "        for method, features in self.selected_features.items():\n",
    "            clf = RandomForestClassifier(n_estimators=100)\n",
    "            clf.fit(X_train[features], y_train)\n",
    "            y_pred = clf.predict(X_test[features])\n",
    "            metrics = {\n",
    "                \"Method\": method,\n",
    "                \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"F1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "                \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "                \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "                \"Selected_Features\": \", \".join(features)\n",
    "            }\n",
    "            metrics_list.append(metrics)\n",
    "        \n",
    "        self.results = pd.DataFrame(metrics_list)\n",
    "        print(\"Evaluation completed. Metrics stored.\")\n",
    "\n",
    "    def generate_report(self, output_path=\"wine_feature_report.xlsx\"):\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            self.results.to_excel(writer, index=False, sheet_name='Metrics')\n",
    "            \n",
    "            # Save selected features per method\n",
    "            features_df = pd.DataFrame(dict([(k, pd.Series(v)) for k,v in self.selected_features.items()]))\n",
    "            features_df.to_excel(writer, index=False, sheet_name='Selected_Features')\n",
    "        \n",
    "        print(f\"Report generated at {output_path}\")\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        # Combined importance visualization (optional)\n",
    "        combined_features = set(sum(self.selected_features.values(), []))\n",
    "        importance_counts = {feat: sum(feat in v for v in self.selected_features.values()) for feat in combined_features}\n",
    "        importance_series = pd.Series(importance_counts).sort_values(ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=importance_series.values, y=importance_series.index)\n",
    "        plt.title(\"Feature Selection Frequency Across Methods\")\n",
    "        plt.xlabel(\"Number of Methods Selected\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "\n",
    "# Convert target to binary classification (quality >=6 as good)\n",
    "df['quality'] = (df['quality'] >= 6).astype(int)\n",
    "\n",
    "pipeline = FeatureSelectionPipeline(df, target='quality')\n",
    "pipeline.preprocess()\n",
    "pipeline.filter_methods()\n",
    "pipeline.wrapper_methods()\n",
    "pipeline.embedded_methods()\n",
    "pipeline.evaluate_models()\n",
    "pipeline.generate_report()\n",
    "pipeline.plot_feature_importance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
